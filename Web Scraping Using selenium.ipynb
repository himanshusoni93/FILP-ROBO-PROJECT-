{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install the selenium \n",
    "\n",
    "##!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic libraries :=\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.naukri.com/data-scientist-jobs-in-delhi?k=data%20scientist&l=delhi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the 4 empty lists\n",
    "\n",
    "job_title=[]\n",
    "company_name=[]\n",
    "locations=[]\n",
    "expriences=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for move all the 10 pages (NEXT)\n",
    "## we use the for loop\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets extect the tag's haveing job titles\n",
    "\n",
    "title_tag = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "\n",
    "title_tag[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now extect the text for the tegs\n",
    "\n",
    "for i in title_tag:\n",
    "    \n",
    "    job_title.append(i.text)\n",
    "    \n",
    "job_title[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets extect the tag's haveing company name \n",
    "\n",
    "company_tag = driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "company_tag[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now extect the text for the company_tag\n",
    "\n",
    "for i in company_tag:\n",
    "    \n",
    "    company_name.append(i.text)\n",
    "    \n",
    "company_name[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets extect the tag's haveing location_tag\n",
    "\n",
    "location_tag = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]')\n",
    "\n",
    "location_tag[0:4]\n",
    "\n",
    "\n",
    "## now extect the text for the location_tag\n",
    "\n",
    "for i in location_tag:\n",
    "    \n",
    "    locations.append(i.text)\n",
    "    \n",
    "locations[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets extect the tag's haveing expriences\n",
    "\n",
    "expriences_tag = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]')\n",
    "\n",
    "expriences_tag[0:4]\n",
    "\n",
    "\n",
    "## now extect the text for the company_tag\n",
    "\n",
    "for i in expriences_tag:\n",
    "    \n",
    "    expriences.append(i.text)\n",
    "    \n",
    "expriences[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the length of all the lists\n",
    "print(len(job_title),len(company_name),len(locations),len(expriences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Naukri_job = pd.DataFrame({})\n",
    "Naukri_job[\"job_title\"]=job_title\n",
    "Naukri_job[\"company_name\"]=company_name\n",
    "Naukri_job[\"locations\"]=locations\n",
    "Naukri_job[\"expriences\"]=expriences\n",
    "\n",
    "\n",
    "Naukri_job.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move one page to other with code and scrap the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic libraries :=\n",
    "\n",
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.naukri.com/data-scientist-jobs-in-delhi?k=data%20scientist&l=delhi\"\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "    \n",
    "## create the 4 empty lists\n",
    "\n",
    "jobs_title=[]\n",
    "companies_name=[]\n",
    "locations_list=[]\n",
    "expriences_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## be using the for loop we move all the 10 pages and extect the data for the pages \n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(0,10):\n",
    "    jobs_tag=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') # for join titles\n",
    "    for i in jobs_tag:\n",
    "        jobs_title.append(i.text)\n",
    "        \n",
    "    companies_tag=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]') # for companies names \n",
    "    for i in companies_tag:\n",
    "        companies_name.append(i.text)\n",
    "        \n",
    "    locations_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]') # for location names\n",
    "    for i in locations_tag:\n",
    "        locations_list.append(i.text)\n",
    "        \n",
    "    expriences_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]') # for year of expriences\n",
    "    for i in expriences_tag:\n",
    "        expriences_list.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//div[@class='pagination mt-64 mb-60']/a[2]\").click()\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(head_line_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the length of all the lists\n",
    "print(len(jobs_title),len(companies_name),len(locations_list),len(expriences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Naukri_jobs = pd.DataFrame({})\n",
    "Naukri_jobs[\"jobs_title\"]=jobs_title\n",
    "Naukri_jobs[\"companies_name\"]=companies_name\n",
    "Naukri_jobs[\"locations_name\"]=locations_list\n",
    "Naukri_jobs[\"expriences_year\"]=expriences_list\n",
    "\n",
    "\n",
    "Naukri_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple pages and there content \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Url = \"https://www.dailypioneer.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first connect to the web driver \n",
    "\n",
    "Driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Driver.get(Url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## accessing archives page\n",
    "\n",
    "archives = Driver.find_element_by_xpath(\"//section[@class='section bluebg footerLinks sectionEleven']/div/div/div/ul/li[5]/a\")\n",
    "try:\n",
    "    archives.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(archives.get_attribute(\"herf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrap the perticuler month\n",
    "\n",
    "month=Driver.find_element_by_xpath(\"/html/body/div[3]/main/section[2]/div/div/div[1]/div/div/div[2]/div[2]/div/ul/li[2]/a\")\n",
    "try:\n",
    "    month.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(month.get_attribute(\"herf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the empty lists \n",
    "\n",
    "headline = []\n",
    "detailed_news = []\n",
    "date = []\n",
    "author = []\n",
    "vertical = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## now we are going make a list which contains the urls of next three page\n",
    "\n",
    "URL = []\n",
    "page_buttom = Driver.find_elements_by_xpath(\"//div[@class='pagingList']/ul/li/a\")\n",
    "for i in page_buttom[1:3]:\n",
    "    URL.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scraping the headline of the web page:-\n",
    "\n",
    "Head = Driver.find_elements_by_xpath(\"//div[@class='row newsWrap no-gutters']/div/h2/a\")\n",
    "Head[0:3]\n",
    "\n",
    "for i in Head:\n",
    "    headline.append(i.text)\n",
    "    \n",
    "headline[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scraping the headline URL\n",
    "\n",
    "Head_URl = []\n",
    "for i in Head:\n",
    "    \n",
    "    Head_URl.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "Head_URl[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for only first page\n",
    "\n",
    "\n",
    "for i in Head_URl:\n",
    "        Driver.get(i)\n",
    "        try: # scraping the verticals\n",
    "            vertical.append(Driver.find_elements_by_xpath(\"//section[@class='section navigateSection animfadeIn pt-sm-4 pb-sm-4']/div/div/div/ul/il/il/a\"))\n",
    "        except NoSuchElementException: # hendling no such element Exception\n",
    "            vertical.append(\"No description\")\n",
    "        try:# scraping the discription \n",
    "            text = Driver.find_elements_by_xpath(\"//div[@class='newsDetailedContent']\")\n",
    "            detailed_news.append(text)\n",
    "        except NoSuchElementException:\n",
    "            detailed_news.append(\"No description\")\n",
    "        try:## scraping the date\n",
    "            date.append(Driver.find_elements_by_xpath(\"//span[@itemprop='datePublised']\")) \n",
    "        except NoSuchElementException:\n",
    "            date.append(\"No Details avaliable\")\n",
    "        try:## scraping the author\n",
    "            author.append(Driver.find_elements_by_xpath(\"//span[@itemprop='author']\"))\n",
    "        except NoSuchElementException:\n",
    "            author.append(\"No Details avaliable\")\n",
    "       \n",
    "            \n",
    "        \n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df =pd.DataFrame({\"Headline\":headline[0:2],\n",
    "               \"Date\":date[0:2],\n",
    "               \"Author\":author[0:2],\n",
    "               \"Vertical\":vertical[0:2],\n",
    "               \"Discription\":detailed_news[0:2]})\n",
    "\n",
    "df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Tabular Data with Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "## spacifiying the url\n",
    "URL = \"https://www.basketball-reference.com/players/i/irvinky01.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## where requests.get(URL) is basically getting the information from the page and BeautifulSoup(page.content, ‘html.parser’) \n",
    "##is to parse the information.\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "## the tables on the page always start with <table …> and end with </table> (highlighted by blue in the plot above). \n",
    "##And these tables are exactly what I want from the page.\n",
    "tables = soup.find_all(\"table\")\n",
    "## We then can apply the find_all function to the parsed information in soup. soup.find_all(“table”) is collecting all the \n",
    "#blocks of information that start with <table> and end with </table>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For each of the table in the variable tables, usually, the table headers start with <th> \n",
    "##and all the table cells of the rows start with <td>. So, the table can be extracted and \n",
    "#converted to pandas data frame in the following code.table = tables[0]\n",
    "\n",
    "table = tables[0]\n",
    "tab_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "df = pd.DataFrame(tab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#To move the first row to the headers, simply type\n",
    "df.columns = df.iloc[0,:]\n",
    "df.drop(index=0,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To get all the tables of the page in the same way as the first table (tables[0]), \n",
    "##I created a dictionary and use the attribute ‘id’ of each table as the key within the for-loop.\n",
    "\n",
    "tabs_dic = {}\n",
    "    \n",
    "for table in tables:\n",
    "    tab_name = table['id']\n",
    "        \n",
    "    tab_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "                        for row in table.find_all(\"tr\")]\n",
    "    df = pd.DataFrame(tab_data)\n",
    "    df.columns = df.iloc[0,:]\n",
    "    df.drop(index=0,inplace=True)\n",
    "        \n",
    "    #df = df.loc[df.Season != \"\"]\n",
    "    tabs_dic[tab_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tabs_of_player_page(URL = 'https://www.basketball-reference.com/players/i/irvinky01.html'):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(URL)\n",
    "    soup = BeautifulSoup(driver.page_source,'html')\n",
    "    driver.quit()\n",
    "    tables = soup.find_all('table',{\"class\":[\"row_summable sortable stats_table now_sortable\",\"suppress_all sortable stats_table now_sortable\",\"sortable stats_table now_sortable\",\"suppress_glossary sortable stats_table now_sortable\"]})\n",
    "    tabs_dic = {}\n",
    "    \n",
    "    for table in tables:\n",
    "        tab_name = table['id']\n",
    "        \n",
    "        tab_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])] for row in table.find_all(\"tr\")]\n",
    "        df = pd.DataFrame(tab_data)\n",
    "        df.columns = df.iloc[0,:]\n",
    "        df.drop(index=0,inplace=True)\n",
    "        \n",
    "        tabs_dic[tab_name] = df\n",
    "    \n",
    "    return tabs_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
