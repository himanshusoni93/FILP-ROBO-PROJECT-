{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-1\n",
    "\n",
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.naukri.com/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"sugInp\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[1]/div/div/div/div[1]/div[2]/input\").send_keys(\"Data Analyst\")\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[2]/div/div/div/div[1]/div[2]/input\").send_keys(\"Bangalore\")\n",
    "\n",
    "# now click the search button.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[3]/button\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "    \n",
    "## create the 4 empty lists\n",
    "\n",
    "jobs_title=[]\n",
    "companies_name=[]\n",
    "job_location=[]\n",
    "expriences=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## be using the for loop we move all the 10 pages and extect the data for the pages \n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(0,5):\n",
    "    jobs_tag=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') # for join titles\n",
    "    for i in jobs_tag:\n",
    "        jobs_title.append(i.text)\n",
    "        \n",
    "    companies_tag=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]') # for companies names \n",
    "    for i in companies_tag:\n",
    "        companies_name.append(i.text)\n",
    "        \n",
    "    locations_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]') # for location names\n",
    "    for i in locations_tag:\n",
    "        job_location.append(i.text)\n",
    "        \n",
    "    expriences_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]') # for year of expriences\n",
    "    for i in expriences_tag:\n",
    "        expriences.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//div[@class='pagination mt-64 mb-60']/a[2]\").click()\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the length of all the lists\n",
    "print(len(jobs_title),len(companies_name),len(job_location),len(expriences))\n",
    "\n",
    "print()\n",
    "\n",
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Naukri_jobs = pd.DataFrame({})\n",
    "Naukri_jobs[\"jobs_title\"]=jobs_title\n",
    "Naukri_jobs[\"companies_name\"]=companies_name\n",
    "Naukri_jobs[\"locations_name\"]=job_location\n",
    "Naukri_jobs[\"expriences_year\"]=expriences\n",
    "\n",
    "\n",
    "Naukri_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 2 :-\n",
    "\n",
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.naukri.com/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"sugInp\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[1]/div/div/div/div[1]/div[2]/input\").send_keys(\"Data Scientist\")\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[2]/div/div/div/div[1]/div[2]/input\").send_keys(\"Bangalore\")\n",
    "\n",
    "# now click the search button.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[3]/button\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "jobs_title=[]\n",
    "companies_name=[]\n",
    "locations_list=[]\n",
    "expriences_list=[]\n",
    "job_description=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## be using the for loop we move all the 10 pages and extect the data for the pages \n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(0,4):\n",
    "    jobs_tag=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') # for join titles\n",
    "    for i in jobs_tag:\n",
    "        jobs_title.append(i.text)\n",
    "        \n",
    "    companies_tag=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]') # for companies names \n",
    "    for i in companies_tag:\n",
    "        companies_name.append(i.text)\n",
    "        \n",
    "    locations_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]') # for location names\n",
    "    for i in locations_tag:\n",
    "        locations_list.append(i.text)\n",
    "        \n",
    "    expriences_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]') # for year of expriences\n",
    "    for i in expriences_tag:\n",
    "        expriences_list.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//div[@class='pagination mt-64 mb-60']/a[2]\").click()\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Naukri_jobs = pd.DataFrame({})\n",
    "Naukri_jobs[\"jobs_title\"]=jobs_title\n",
    "Naukri_jobs[\"companies_name\"]=companies_name\n",
    "Naukri_jobs[\"locations_name\"]=locations_list\n",
    "Naukri_jobs[\"expriences_year\"]=expriences_list\n",
    "\n",
    "\n",
    "Naukri_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:-2-(1)\n",
    "\n",
    "## :==> Please note that you have to scrape full job description. For that you may have to open each job separately as shown below\n",
    "\n",
    "## :==> You have to click on each job title. Here in this case if you will click on “ Data Scientist, Machine Learning” , you will land up on the below page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.naukri.com/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"sugInp\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[1]/div/div/div/div[1]/div[2]/input\").send_keys(\"Data Scientist\")\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[2]/div/div/div/div[1]/div[2]/input\").send_keys(\"Bangalore\")\n",
    "\n",
    "# now click the search button.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[3]/button\").click()\n",
    "\n",
    "\n",
    "## create the 2 empty lists\n",
    "\n",
    "jobs_title=[]\n",
    "job_description=[]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scraping the jobs_tags of the web page:-\n",
    "\n",
    "jobs_tags = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "jobs_tags[0:3]\n",
    "\n",
    "for i in jobs_tags:\n",
    "    jobs_title.append(i.text)\n",
    "    \n",
    "jobs_title[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## scraping the jobs_tags URL\n",
    "\n",
    "jobs_title_URl = []\n",
    "                                            \n",
    "for i in jobs_tags:\n",
    "    jobs_title_URl.append(i.get_attribute(\"href\"))\n",
    "jobs_title_URl[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs_title_URl:\n",
    "        driver.get(i)\n",
    "        try:# scraping the discription \n",
    "            text = driver.find_elements_by_xpath(\"/html/body/div[1]/main/div[2]/div[2]/section[2]/div[1]/p[2]\")\n",
    "            for x in text:\n",
    "                job_description.append(x.text)\n",
    "        except NoSuchElementException:\n",
    "            job_description.append(\"No description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(jobs_title),len(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_Name = jobs_title[0:9]\n",
    "job_descriptions = job_description[0:9]\n",
    "companies_name1 =companies_name[0:9]\n",
    "locations_list1 = locations_list[0:9]\n",
    "expriences_list1 = expriences_list[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the data frame :(Run first part first )\n",
    "\n",
    "df = pd.DataFrame({\"jobs_title\":job_Name,\n",
    "                   \"companies_name\":companies_name1,\n",
    "                   \"locations_name\":locations_list1,\n",
    "                   \"expriences_year\":expriences_list1,\n",
    "                  \"job_description\":job_descriptions})\n",
    "\n",
    "df[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-3\n",
    "\n",
    "## :- In this question you have to scrape data using the filters available on the webpage as shown below:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q3:- In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    ":-You have to use the location and salary filter.\n",
    ":-You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    ":-You have to scrape the job-title, job-location, company_name, experience_required.\n",
    ":-The location filter to be used is “Delhi/NCR”\n",
    ":-The salary filter to be used is “3-6” lakhs\n",
    ":-The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.naukri.com/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"sugInp\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[1]/div/div/div/div[1]/div[2]/input\").send_keys(\"Data Scientist\")\n",
    "\n",
    "\n",
    "# now click the search button.\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section/div/form/div[3]/button\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "## accessing filer location(Delhi/NCR) page\n",
    "\n",
    "location = driver.find_element_by_xpath(\"//span[@title='Delhi/NCR']\")\n",
    "\n",
    "try:\n",
    "    location.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(location.get_attribute(\"herf\"))\n",
    "    \n",
    "time.sleep(3)    \n",
    "### Scrap the perticuler salary (“3-6” lakhs)\n",
    "\n",
    "salary = driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\")\n",
    "try:\n",
    "    salary.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(salary.get_attribute(\"herf\"))\n",
    "\n",
    "    \n",
    "time.sleep(3)    \n",
    "## Create the Empty list\n",
    "job_title = [] \n",
    "job_location = [] \n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "\n",
    "\n",
    "jobs_tag=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') # for join titles\n",
    "for i in jobs_tag:\n",
    "    job_title.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "companies_tag=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]') # for companies names \n",
    "for i in companies_tag:\n",
    "    company_name.append(i.text)\n",
    "        \n",
    "    \n",
    "locations_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]') # for location names\n",
    "for i in locations_tag:\n",
    "    job_location.append(i.text)\n",
    "        \n",
    "    \n",
    "expriences_tag=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]') # for year of expriences\n",
    "for i in expriences_tag:\n",
    "    experience_required.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "## create the dataframe \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Naukri_jobs = pd.DataFrame({})\n",
    "Naukri_jobs[\"jobs_title\"]=job_title\n",
    "Naukri_jobs[\"companies_name\"]=company_name\n",
    "Naukri_jobs[\"locations_name\"]=job_location\n",
    "Naukri_jobs[\"expriences_year\"]=experience_required\n",
    "\n",
    "\n",
    "Naukri_jobs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-4\n",
    "\n",
    "## Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "url = (\"https://www.glassdoor.co.in/index.htm\")\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "sign_in=driver.find_element_by_xpath(\"//a[@class='track-click gd-btn-locked-transparent susiLink sign-in strong nowrap']\")\n",
    "sign_in.click()\n",
    "time.sleep(3)\n",
    "## enter the email-id and password\n",
    "driver.find_element_by_id(\"userEmail\").send_keys(\"testidforweb@gmail.com\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"userPassword\").send_keys(\"aakash123**\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath('//button[@class=\"gd-ui-button minWidthBtn css-8i7bc2\"]').click()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"sc.keyword\").send_keys(\"Data Scientist\")\n",
    "driver.find_element_by_id(\"sc.location\").send_keys(\"NOIDA\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"sc.location\").send_keys(Keys.ENTER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the 4 empty lists\n",
    "\n",
    "jobs_title=[]\n",
    "companies_name=[]\n",
    "job_location=[]\n",
    "salary=[]\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "jobs_tag=driver.find_elements_by_xpath(\"//a[@class='jobLink css-1rd3saf eigr9kq2']\")\n",
    "for i in jobs_tag:\n",
    "    jobs_title.append(i.text)\n",
    "    \n",
    "time.sleep(2)\n",
    "        \n",
    "companies_tag=driver.find_elements_by_xpath(\"//a[@class=' css-l2wjgv e1n63ojh0 jobLink']\") # for companies names \n",
    "for i in companies_tag:\n",
    "    companies_name.append(i.text)\n",
    "companies_name[0:4]\n",
    "    \n",
    "time.sleep(2) \n",
    "\n",
    "locations_tag=driver.find_elements_by_xpath(\"//span[@class='pr-xxsm css-1ndif2q e1rrn5ka0']\") # for location names\n",
    "for i in locations_tag:\n",
    "    job_location.append(i.text)\n",
    "job_location[0:3]\n",
    "    \n",
    "time.sleep(2)\n",
    "        \n",
    "salary_tag=driver.find_elements_by_xpath(\"//span[@class='css-1imh2hq e1wijj242']\") # for post_days\n",
    "for i in salary_tag:\n",
    "    salary.append(i.text)\n",
    "        \n",
    "time.sleep(2)       \n",
    "## print the length of all the lists\n",
    "print(len(jobs_title),len(companies_name),len(job_location),len(salary))\n",
    "\n",
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Glassdoor_jobs = pd.DataFrame({})\n",
    "Glassdoor_jobs[\"jobs_title\"]=jobs_title[0:10]\n",
    "Glassdoor_jobs[\"companies_name\"]=companies_name[0:10]\n",
    "Glassdoor_jobs[\"locations_name\"]=job_location[0:10]\n",
    "Glassdoor_jobs[\"salary\"]=salary[0:10]\n",
    "\n",
    "\n",
    "Glassdoor_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 5\n",
    "\n",
    "## Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page\n",
    "You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company.\n",
    "6.Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "url = (\"https://www.glassdoor.co.in/index.htm\")\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "sign_in=driver.find_element_by_xpath(\"//a[@class='track-click gd-btn-locked-transparent susiLink sign-in strong nowrap']\")\n",
    "sign_in.click()\n",
    "time.sleep(3)\n",
    "## enter the email-id and password\n",
    "driver.find_element_by_id(\"userEmail\").send_keys(\"testidforweb@gmail.com\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"userPassword\").send_keys(\"aakash123**\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath('//button[@class=\"gd-ui-button minWidthBtn css-8i7bc2\"]').click()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"sc.keyword\").send_keys(\"Data Scientist\")\n",
    "driver.find_element_by_id(\"sc.location\").send_keys(\"NOIDA\")\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"sc.location\").send_keys(Keys.ENTER);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the 4 empty lists\n",
    "time.sleep(2)\n",
    "companies_name=[]\n",
    "Max_Salary=[]\n",
    "Mini_Salary=[]\n",
    "Avg_Salary=[]\n",
    "Ratings=[]\n",
    "\n",
    "    \n",
    "time.sleep(2) \n",
    "\n",
    "\n",
    "companies_tag=driver.find_elements_by_xpath(\"//P[@class='m-0 ']\") # for companies names \n",
    "for i in companies_tag:\n",
    "    companies_name.append(i.text)\n",
    "    \n",
    "time.sleep(2)\n",
    "\n",
    "Max_Salary_tag=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")## Max Salary \n",
    "for i in Max_Salary_tag:\n",
    "    Max_Salary.append(i.text)\n",
    "    \n",
    "time.sleep(2)\n",
    "        \n",
    "\n",
    "Mini_Salary_tag=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[1]\") # for Mini salary names\n",
    "for i in Mini_Salary_tag:\n",
    "    Mini_Salary.append(i.text)\n",
    "    \n",
    "time.sleep(2)\n",
    "        \n",
    "Avg_Salary_tag=driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']/strong[1]\") # for Avg salary\n",
    "for i in Avg_Salary_tag:\n",
    "    Avg_Salary.append(i.text)\n",
    "    \n",
    "time.sleep(2)\n",
    "\n",
    "           \n",
    "## print the length of all the lists\n",
    "print(len(companies_name),len(Max_Salary),len(Mini_Salary),len(Avg_Salary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Glassdoor_jobs = pd.DataFrame({})\n",
    "Glassdoor_jobs[\"companies_name\"]=companies_name[0:10]\n",
    "Glassdoor_jobs[\"Max_Salary\"]=Max_Salary[0:10]\n",
    "Glassdoor_jobs[\"Mini_Salary\"]=Mini_Salary[0:10]\n",
    "Glassdoor_jobs[\"Avg_Salary\"]=Avg_Salary[0:10]\n",
    "\n",
    "\n",
    "Glassdoor_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question :-6 \n",
    "\n",
    "## Q6 : Scrape data of first 100 sunglasses listings on flipkart.com.You have to scrape four attributes:\n",
    "### 1. Brand // 2. Product Description // 3. Price // 4. Discount %\n",
    "#### The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.flipkart.com/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"_3704LK\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “sunglasses”  and enter y\n",
    "driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys(\"sunglasses\")\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
    "## click on the search button \n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "## create the empty list\n",
    "\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "Discount = []\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "for i in range(0,5):\n",
    "    Brand_tag=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "    for i in Brand_tag:\n",
    "        Brand.append(i.text)\n",
    "          \n",
    "    Product_Description_tag=driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]') # for companies names \n",
    "    for i in Product_Description_tag:\n",
    "        Product_Description.append(i.text)\n",
    "        \n",
    "        \n",
    "    Price_tag=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]') # for year of expriences\n",
    "    for i in Price_tag:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    Discount_tag=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]/span') # for location names\n",
    "    for i in Discount_tag:\n",
    "        Discount.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//div[@class='_2MImiq']/span\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "## print the length of the lists    \n",
    "print(len(Brand),len(Product_Description),len(Price),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the Dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Flipkart_ws = pd.DataFrame({})\n",
    "Flipkart_ws[\"Brand\"]=Brand[0:100]\n",
    "Flipkart_ws[\"Product_Description\"]=Product_Description[0:100]\n",
    "Flipkart_ws[\"Price\"]=Price[0:100]\n",
    "Flipkart_ws[\"Discount\"]=Discount[0:100]\n",
    "\n",
    "\n",
    "Flipkart_ws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qustion-7\n",
    "\n",
    "### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "####When you will open the above link you will reach to the below shown webpage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "###### Click on the View all \n",
    "\n",
    "driver.find_element_by_xpath('//div[@class=\"_3UAT2v _16PBlm\"]/span').click()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the empty folder \n",
    "\n",
    "Ratings = []\n",
    "Review_summary = []\n",
    "Full_review = []\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(0,4):\n",
    "    Ratings_tag=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in Ratings_tag:\n",
    "        Ratings.append(i.text)\n",
    "          \n",
    "    Review_summary_tag=driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]') # for companies names \n",
    "    for i in Review_summary_tag:\n",
    "        Review_summary.append(i.text)\n",
    "        \n",
    "    Full_review_tag=driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]') # for year of expriences\n",
    "    for i in Full_review_tag:\n",
    "        Full_review.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath('//div[@class=\"_2MImiq _1Qnn1K\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "print(len(Ratings),len(Review_summary),len(Full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Dateframe \n",
    "import pandas as pd \n",
    "\n",
    "I_phones = pd.DataFrame({})\n",
    "I_phones[\"Ratings\"]=Ratings\n",
    "I_phones[\"Review_summary\"]=Review_summary\n",
    "I_phones[\"Full_review\"]=Full_review\n",
    "\n",
    "I_phones.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quetion-8\n",
    "\n",
    "\n",
    "### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.flipkart.com/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"_3704LK\")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “sunglasses”  and enter y\n",
    "driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys(\"sneakers\")\n",
    "\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()\n",
    "## click on the search button \n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "## create the empty list\n",
    "\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "Discount = []\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(0,5):\n",
    "    Brand_tag=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "    for i in Brand_tag:\n",
    "        Brand.append(i.text)\n",
    "          \n",
    "    Product_Description_tag=driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]') # for companies names \n",
    "    for i in Product_Description_tag:\n",
    "        Product_Description.append(i.text)\n",
    "        \n",
    "        \n",
    "    Price_tag=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]') # for year of expriences\n",
    "    for i in Price_tag:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    Discount_tag=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]/span') # for location names\n",
    "    for i in Discount_tag:\n",
    "        Discount.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"//div[@class='_2MImiq']/span\").click()\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand),len(Product_Description),len(Price),len(Discount))\n",
    "\n",
    "## create the Dataframe \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sneaker_ws = pd.DataFrame({})\n",
    "sneaker_ws[\"Brand\"]=Brand[0:100]\n",
    "sneaker_ws[\"Product_Description\"]=Product_Description[0:100]\n",
    "sneaker_ws[\"Price\"]=Price[0:100]\n",
    "sneaker_ws[\"Discount\"]=Discount[0:100]\n",
    "\n",
    "sneaker_ws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qustion:-9\n",
    "\n",
    "### Q9: Go to the link - https://www.myntra.com/shoes\n",
    "1-Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image\n",
    "2-And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes ,\n",
    "3- Short Shoe description, price of the shoe as shown in the below image.\n",
    "4-Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.myntra.com/shoes\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "### Scrap the perticuler Color filter to “Black\"\n",
    "\n",
    "Color_filter = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "try:\n",
    "    Color_filter.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(Color_filter.get_attribute(\"herf\"))\n",
    "\n",
    "\n",
    "time.sleep(4) \n",
    "\n",
    "## accessing filer Set Price filter\n",
    "\n",
    "Price_filter = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div\")\n",
    "\n",
    "try:\n",
    "    Price_filter.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(Price_filter.get_attribute(\"herf\"))\n",
    "    \n",
    "\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "## Create the Empty list\n",
    "Shoes_Brand = [] \n",
    "Shoes_Description = []\n",
    "Price = []\n",
    "Discount = [] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "\n",
    "for i in range(0,5):\n",
    "    Shoes_Brand_tag=driver.find_elements_by_xpath('//h3[@class=\"product-brand\"]')## for  Shoes_Brand\n",
    "    for i in Shoes_Brand_tag:\n",
    "        Shoes_Brand.append(i.text)\n",
    "          \n",
    "    Shoes_Description_tag=driver.find_elements_by_xpath('//h4[@class=\"product-product\"]') # for Shoes_Description \n",
    "    for i in Shoes_Description_tag:\n",
    "        Shoes_Description.append(i.text)\n",
    "        \n",
    "        \n",
    "    Price_tag=driver.find_elements_by_xpath('//span[@class=\"product-discountedPrice\"]') # for  Price\n",
    "    for i in Price_tag:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    Discount_tag=driver.find_elements_by_xpath('//span[@class=\"product-discountPercentage\"]') # for Discount\n",
    "    for i in Discount_tag:\n",
    "        Discount.append(i.text)\n",
    "        \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "## print the length of the lists    \n",
    "print(len(Shoes_Brand),len(Shoes_Description),len(Price),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "Shoes_ws = pd.DataFrame({})\n",
    "Shoes_ws[\"Shoes_Brand\"]=Shoes_Brand[0:100]\n",
    "Shoes_ws[\"Shoes_Description\"]=Shoes_Description[0:100]\n",
    "Shoes_ws[\"Price\"]=Price[0:100]\n",
    "Shoes_ws[\"Discount\"]=Discount[0:100]\n",
    "\n",
    "\n",
    "Shoes_ws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-10\n",
    "\n",
    "Go to webpage https://www.amazon.in/\n",
    "\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "\n",
    "## lets connect with web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:/Users/Acer/chromedriver/chromedriver.exe\")\n",
    "\n",
    "## spacifiying the url\n",
    "\n",
    "Url=\"https://www.amazon.in/\"\n",
    "\n",
    "\n",
    "## lets open the web page through our web page \n",
    "driver.get(Url)\n",
    "\n",
    "## check the All input box ir the page\n",
    "\n",
    "inputboxs = driver.find_elements(By.CLASS_NAME,\"nav-search-field \")\n",
    "print(len(inputboxs))\n",
    "\n",
    "## Enter “sunglasses”  and enter y\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\").send_keys(\"Laptop\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "### Enter the  on search button\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "### Scrap the perticuler CPU Type filter to “Intel Core i7\"\n",
    "Intel_Core_i7 = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[2]/li[16]/span\")\n",
    "try:\n",
    "    Intel_Core_i7.click()\n",
    "except ElementNotInteractableException:\n",
    "    drive.get(Intel_Core_i7.get_attribute(\"herf\"))\n",
    "\n",
    "time.sleep(4) \n",
    "\n",
    "### Scrap the perticuler CPU Type filter to “Intel Core i9\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intel_Core_i9 = driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div[1]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[18]/span/a/span\")\n",
    "try:\n",
    "    Intel_Core_i9.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(Intel_Core_i9.get_attribute(\"herf\"))\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "## Create the Empty list\n",
    "Laptop_Brand = [] \n",
    "Ratings = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for j in range(0,5,1):\n",
    "    Laptop_Brand_tag=driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]') # for Laptop_Brand\n",
    "    for i in Laptop_Brand_tag:\n",
    "        Laptop_Brand.append(i.text)\n",
    "          \n",
    "    Ratings_tag=driver.find_elements_by_xpath('//span[@class=\"a-icon-alt\"]') # for Ratings \n",
    "    for i in Ratings_tag:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "        \n",
    "    Price_tag=driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]') # forPrice\n",
    "    for i in Price_tag:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "        \n",
    "    driver.find_element_by_xpath('//ul[@class=\"a-pagination\"]').click()\n",
    "    \n",
    "    time.sleep(4)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "laptop_ws = pd.DataFrame({})\n",
    "laptop_ws[\"Title\"]=Laptop_Brand[0:50]\n",
    "laptop_ws[\"Ratings\"]=Ratings[0:50]\n",
    "laptop_ws[\"Price\"]=Price[0:50]\n",
    "\n",
    "\n",
    "\n",
    "laptop_ws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "WEB SCRAPING ASSIGNMENT-2.ipynb",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "356px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
