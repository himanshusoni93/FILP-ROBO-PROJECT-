{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification and Analysis (TIMES_OF_INDIA DEC 2018_ FEB 2019 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM STATEMENT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News archives contain several Gigabytes of data, which is not easy to handle. For simplicity they are stored in simplest format possible. But during analysis, analysts require a meta information about each news article. Sometimes it is available but there are cases where meta information is missing. This is where data science comes into play. Use cases are built to analyse the available meta data and hence analyse meta information about the news articles, where it is not present. In this project we have to build such use case where we can generate the metadata around news articles and make it ready for further analysis. In phase 1, we need to gather information from archives of past few years from different news websites. In this phase we will scrape the news websites for news and related information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task assigned to you is to scrape archives from The Times of India website for years DEC-2018 to FEB-2019. You will be allotted tasks to scrape data based on different timeframes. The details to be scraped are given below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Date: This column will contain the date on which news was reported.\n",
    "2. Author: This column will contain name of the author of news report.\n",
    "3. Vertical: This column will contain the news verticals (e.g. Entertainment, Sports, Lifestyle, Economy, etc.).\n",
    "4. Headline: This column will contain the news headline.\n",
    "5. Description: This column will contain the main content of the news report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the basic Libraries import requests\n",
    "\n",
    "import numpy as np, pandas  as pd,time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Safari\n",
    "from selenium.webdriver import DesiredCapabilities\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_of_india(url):\n",
    "    \n",
    "    times_of_india_dict={}\n",
    "    \n",
    "    chromedriver='C:/Users/Acer/chromedriver/chromedriver.exe'\n",
    "    op = webdriver.ChromeOptions()\n",
    "    op.add_argument('headless')\n",
    "    browser = webdriver.Chrome(chromedriver,options=op)\n",
    "    \n",
    "    times_of_india_dict.update({'Date':[]})\n",
    "    times_of_india_dict.update({'Author':[]})\n",
    "    times_of_india_dict.update({'Vertical':[]})\n",
    "    times_of_india_dict.update({'Headline':[]})\n",
    "    times_of_india_dict.update({'Description':[]})\n",
    "    \n",
    "    dec_url=url+'/archive/year-2018,month-12.cms'\n",
    "    browser.get(dec_url)\n",
    "    time.sleep(1)\n",
    "    dec_page=browser.page_source\n",
    "    dec_page_soup=BeautifulSoup(dec_page,'lxml')\n",
    "    \n",
    "    dec_calender=dec_page_soup.find_all('table',attrs={'id':'calender'})[0].find_all('a')\n",
    "    \n",
    "    for x in range(len(dec_calender)):\n",
    "        if dec_calender[x].text=='':\n",
    "            continue\n",
    "        else:\n",
    "            print(x)\n",
    "            next_date=dec_calender[x].get('href')\n",
    "            next_url=url+next_date\n",
    "            browser.get(next_url)\n",
    "            time.sleep(1)\n",
    "            next_page=browser.page_source\n",
    "            next_soup=BeautifulSoup(next_page,'lxml')\n",
    "            int_page_list=next_soup.find_all('span',attrs={'style':'font-family:arial ;font-size:12;color: #006699'})[0].find_all('a')\n",
    "            for y in range(len(int_page_list)):\n",
    "                try:\n",
    "                    times_of_india_dict['Headline'].append(int_page_list[y].text)\n",
    "                except:\n",
    "                    times_of_india_dict['Headline'].append(None)\n",
    "                try:\n",
    "                    date=next_soup.find_all('b')[0].text\n",
    "                except:\n",
    "                    date=None\n",
    "                news_link=int_page_list[y].get('href')\n",
    "                browser.get(news_link)\n",
    "                time.sleep(1)\n",
    "                news=browser.page_source\n",
    "                news_soup=BeautifulSoup(news,'lxml')\n",
    "                try:\n",
    "                    author=news_soup.find_all('div',attrs={'class':'_3Mkg- byline'})[0].span.a.text\n",
    "                except:\n",
    "                    try:\n",
    "                        author=news_soup.find_all('span',attrs={'class':'auth_detail'})[0].text\n",
    "                    except:\n",
    "                        author=None\n",
    "#                 try:\n",
    "#                     date=news_soup.find_all('div',attrs={'data-plugin':'dateformat'})[0].text.replace('Created:','').replace('Updated:','').strip()\n",
    "#                 except:\n",
    "#                     date=None\n",
    "                try:\n",
    "                    vertical=news_soup.find_all('span',attrs={'itemprop':'name'})[1].text\n",
    "                except:\n",
    "                    vertical=None\n",
    "                try:\n",
    "                    description=news_soup.find_all('div',attrs={'class':'ga-headlines'})[0].text\n",
    "                except:\n",
    "                    try:\n",
    "                        description=news_soup.find_all('div',attrs={'class':'Normal'})[0].text\n",
    "                    except:\n",
    "                        description=None\n",
    "                \n",
    "                times_of_india_dict['Author'].append(author)\n",
    "                times_of_india_dict['Date'].append(date)\n",
    "                times_of_india_dict['Vertical'].append(vertical)\n",
    "                times_of_india_dict['Description'].append(description)\n",
    "            \n",
    "    feb_url=url+'/archive/year-2019,month-2.cms'\n",
    "    browser.get(feb_url)\n",
    "    time.sleep(1)\n",
    "    feb_page=browser.page_source\n",
    "    feb_page_soup=BeautifulSoup(feb_page,'lxml')\n",
    "    \n",
    "    feb_calender=feb_page_soup.find_all('table',attrs={'id':'calender'})[0].find_all('a')\n",
    "    \n",
    "    for x in range(len(feb_calender)):\n",
    "        if feb_calender[x].text=='':\n",
    "            continue\n",
    "        else:\n",
    "            print(x)\n",
    "            next_date=feb_calender[x].get('href')\n",
    "            next_url=url+next_date\n",
    "            browser.get(next_url)\n",
    "            time.sleep(1)\n",
    "            next_page=browser.page_source\n",
    "            next_soup=BeautifulSoup(next_page,'lxml')\n",
    "            int_page_list=next_soup.find_all('span',attrs={'style':'font-family:arial ;font-size:12;color: #006699'})[0].find_all('a')\n",
    "            for y in range(len(int_page_list)):\n",
    "                try:\n",
    "                    times_of_india_dict['Headline'].append(int_page_list[y].text)\n",
    "                except:\n",
    "                    times_of_india_dict['Headline'].append(None)\n",
    "                try:\n",
    "                    date=next_soup.find_all('b')[0].text\n",
    "                except:\n",
    "                    date=None\n",
    "                news_link=int_page_list[y].get('href')\n",
    "                browser.get(news_link)\n",
    "                time.sleep(1)\n",
    "                news=browser.page_source\n",
    "                news_soup=BeautifulSoup(news,'lxml')\n",
    "                try:\n",
    "                    author=news_soup.find_all('div',attrs={'class':'_3Mkg- byline'})[0].span.a.text\n",
    "                except:\n",
    "                    try:\n",
    "                        author=news_soup.find_all('span',attrs={'class':'auth_detail'})[0].text\n",
    "                    except:\n",
    "                        author=None\n",
    "#                 try:\n",
    "#                     date=news_soup.find_all('div',attrs={'data-plugin':'dateformat'})[0].text.replace('Created:','').replace('Updated:','').strip()\n",
    "#                 except:\n",
    "#                     date=None\n",
    "                try:\n",
    "                    vertical=news_soup.find_all('span',attrs={'itemprop':'name'})[1].text\n",
    "                except:\n",
    "                    vertical=None\n",
    "                try:\n",
    "                    description=news_soup.find_all('div',attrs={'class':'ga-headlines'})[0].text\n",
    "                except:\n",
    "                    try:\n",
    "                        description=news_soup.find_all('div',attrs={'class':'Normal'})[0].text\n",
    "                    except:\n",
    "                        description=None    \n",
    "                \n",
    "                times_of_india_dict['Author'].append(author)\n",
    "                times_of_india_dict['Date'].append(date)\n",
    "                times_of_india_dict['Vertical'].append(vertical)\n",
    "                times_of_india_dict['Description'].append(description)\n",
    "    \n",
    "    times_of_india_df=pd.DataFrame(times_of_india_dict)\n",
    "    times_of_india_df.to_csv('TIMES_OF_INDIA_DEC_FEB.csv')\n",
    "    times_of_india_df.to_json('TIMES_OF_INDIA_DEC_FEB.json')\n",
    "    to_xml(times_of_india_df,'TIMES_OF_INDIA_DEC_FEB.xml')\n",
    "    return times_of_india_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xml(df, filename=None, mode='w'):\n",
    "    def row_to_xml(row):\n",
    "        xml = ['<item>']\n",
    "        for i, col_name in enumerate(row.index):\n",
    "            xml.append('  <field name=\"{0}\">{1}</field>'.format(col_name, row.iloc[i]))\n",
    "        xml.append('</item>')\n",
    "        return '\\n'.join(xml)\n",
    "    res = '\\n'.join(df.apply(row_to_xml, axis=1))\n",
    "\n",
    "    if filename is None:\n",
    "        return res\n",
    "    with open(filename, mode) as f:\n",
    "        f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=times_of_india('https://timesofindia.indiatimes.com')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
